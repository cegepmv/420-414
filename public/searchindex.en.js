var relearn_searchindex = [
  {
    "breadcrumb": "",
    "content": "Qu’est-ce que le Cloud ? En général, quand on pense “Cloud”, on pense surtout aux services de stockage, comme Dropbox, iCloud, Google Drive, ou OneDrive.\nMais le Cloud, c’est bien plus que ça : Il offre des services pour vos besoins informatiques, réseaux, base de données, et même ce que qu’on considère généralement comme le “Cloud” : le stockage de fichiers.\nQuelques-uns de ces outils que nous utilisons tous les jours et dont nous ne savions pas qu’ils faisaient partie du cloud :\nMicrosoft 365 Google Workspace Le “Cloud” est une plate-forme qui permet d’avoir une disponibilité à la demande de ressources informatiques virtuelles pour un usage personnel ou professionnel.\nCes ressources sont utilisées sur la base d’un modèle de paiement à l’utilisation (Pay-as-you-go Model) : vous êtes facturés pour ces services seulement quand vous les utilisez.\nL’utilisation d’une infrastructure ou de services “cloud” signifie que vous ne possédez pas physiquement l’infrastructure. Vous la louez à des fournisseurs de service.\nQuelques exemples de services que vous pouvez utiliser dans le nuage :\nVous pouvez les utiliser pour mettre en place un réseau complet, tester des logiciels ou faire de l’analyse des données… tout ce que vous pouvez imaginer : le nuage n’a pas de limites.\nLes avantages de l’infonuagique Go Global in Minutes : Déployer vos applications dans le monde entier d’un simple clic, lancer des environnements entiers en quelques minutes. Pas de dépenses sur l’infrastructure et la maintenance d’un centre de données (Data Center) : Laisse le temps de se concentrer sur le développement d’applications au lieu de gérer le matériel (vous n’êtes pas propriétaire du centre de données utilisé). Économies d’échelle : Les fournisseurs de services (AWS, GCP ou Azure) possèdent une infrastructure immense, ce qui permet de réduire les couts. Pas besoin de deviner la capacité à l’avance : Gain de temps et d’argent Haute disponibilité (High Availability): Les systèmes à haute disponibilité sont conçus pour fonctionner en continu sans défaillance pendant une longue période. Ces systèmes évitent les pertes de service en réduisant ou en gérant les défaillances. Durabilité : Protection des données à long terme (vos données resteront intactes sans être corrompues). Les fournisseurs de service Les plus populaires (dans l’ordre)\nAWS Microsoft Azure Google Cloud (GCP) D’autres fournisseurs, qui offrent aussi des solution pour créer des infrastructures privées :\nRedhat Dell VMWare OpenStack Emplois dans le monde du Cloud Architecte de solution (Solution Architect) Ingénieur Cloud (Cloud Engineer) : Gestion du nuage Cloud Operations Engineer : Porte d’entrée dans le monde du Cloud (surtout pour quelqu’un qui vient de l’assistance technique) Sales Engineer Ingénieur DevOps (DevOps Engineer) : Responsable de la gestion d’outils et services en utilisant une méthodologie DevOps Support Cloud (Cloud Support) : Rôle généralement basé sur les tickets, en contact direct avec un grand nombre de services dans le nuage.",
    "description": "Qu’est-ce que le Cloud ? En général, quand on pense “Cloud”, on pense surtout aux services de stockage, comme Dropbox, iCloud, Google Drive, ou OneDrive.\nMais le Cloud, c’est bien plus que ça : Il offre des services pour vos besoins informatiques, réseaux, base de données, et même ce que qu’on considère généralement comme le “Cloud” : le stockage de fichiers.\nQuelques-uns de ces outils que nous utilisons tous les jours et dont nous ne savions pas qu’ils faisaient partie du cloud :",
    "tags": [],
    "title": "Introduction",
    "uri": "/420-414/1-introduction/index.html"
  },
  {
    "breadcrumb": "Introduction",
    "content": "Les modèles “as-a-Service” L’expression “as-a-Service” signifie généralement qu’un tiers se charge de vous fournir un service de cloud computing pour que vous puissiez vous concentrer sur des aspects plus importants (développement, relation client etc…).\nInfrastructure sur-site (On-Site) Une infrastructure informatique sur site est la solution qui met le plus de responsabilités entre les mains de l’utilisateur et du responsable. Lorsque l’intégralité du matériel et des logiciels se trouve sur site, vous et votre équipe devez gérer, mettre à jour et, si nécessaire, remplacer chaque composant vous-mêmes.Le cloud computing vous permet d’externaliser la gestion d’un, de plusieurs ou de tous les composants de votre infrastructure en vue de vous faire gagner du temps que vous pourrez consacrer à d’autres tâches.\nPaaS Avec le modèle PaaS, ou Platform-as-a-Service, le fournisseur héberge le matériel et les logiciels sur sa propre infrastructure et met à disposition de l’utilisateur une plateforme via Internet, sous la forme d’une solution intégrée, d’une pile de solutions ou d’un service.\nDestiné aux spécialistes du développement et de la programmation : Vous écrivez le code, créez et gérez vos applications, le tout sans avoir à vous préoccuper des mises à jour logicielles ou de la maintenance du matériel. L’environnement de développement et de déploiement vous est fourni.\nExemples :\nAmazon Elastic Beanstalk (AWS) : service d’orchestration pour le déploiement d’applications qui orchestre divers services AWS Google App Engine (GCP) : Une plate-forme pour le développement et l’hébergement d’applications web dans des centres de données gérés par Google. SaaS En utilisant un produit SaaS (Software as a Service), tout est déjà géré pour vous : Application, Données, Durée d’exécution, Intergiciel, OS, Virtualisation, Serveurs, Stockage et le réseau.\nExemples :\nZoom Dropbox : Service de stockage de fichiers. Slack : Service de communication par messagerie instantanée Mailchimp : Service de marketing par courriel, IaaS Avec l’infrastructure en tant que service, en tant qu’utilisateur, vous êtes responsable de la gestion l’application, les données, le moteur d’exécution, l’intergiciel, et le système d’exploitation, tandis que le fournisseur de services gère la virtualisation, les serveurs, le stockage et le réseau.\nExemples :\nAWS Google Cloud (GCP) Microsoft Azure Modèles de déploiement Le modèle privé (Private Cloud): Pour les organisations qui possèdent un réseau/architecture privé. Les ressources (serveurs, bases de données etc…) sont toutes sur place (on-premise). Un nuage privé ne présente aucun des avantages du nuage discutés plus haut, mais offre plus de sécurité à vos données (elles ne transitent pas par un espace public et ne sont pas partagées avec d’autres organisations) Le modèle public (Public Cloud): L’infrastructure appartient à un fournisseur de service (Cloud Service Provider ou CSP) comme Amazon ou Google ou même Microsoft. Ils fournissent une infrastructure pour des particuliers. Il n’y a aucune responsabilité en matière de matériel (ils fournissent tout le matériel). Le modèle hybride (Hybrid Cloud) : Une combinaison de cloud privé et de cloud public. les données sensibles sont généralement stockées localement et se connectent à l’infrastructure infonuagique publique à l’aide de services comme un VPN.",
    "description": "Les modèles “as-a-Service” L’expression “as-a-Service” signifie généralement qu’un tiers se charge de vous fournir un service de cloud computing pour que vous puissiez vous concentrer sur des aspects plus importants (développement, relation client etc…).\nInfrastructure sur-site (On-Site) Une infrastructure informatique sur site est la solution qui met le plus de responsabilités entre les mains de l’utilisateur et du responsable. Lorsque l’intégralité du matériel et des logiciels se trouve sur site, vous et votre équipe devez gérer, mettre à jour et, si nécessaire, remplacer chaque composant vous-mêmes.",
    "tags": [],
    "title": "Modèles Infonuagiques",
    "uri": "/420-414/1-introduction/1-modeles/index.html"
  },
  {
    "breadcrumb": "Introduction",
    "content": "AWS possède une infrastructure immense, avec des centres de données (data centers) déployés sur les quatre coins du globe. AWS a une façon spécifique de séparer logiquement et physiquement son infrastructure pour permettre une disponibilité et une redondance optimale.\nRégions Une région AWS est une zone géographique.\nVous contrôlez la réplication des données entre les régions. La communication entre les régions s’effecture par le biais de l’infrastructure réseau de AWS. Chaque région AWS assure une redondance et une connectivité complète au réseau AWS.\nUne région se compose de deux zones de disponibilité ou plus.\nZones de disponibilité Chaque région compte plusieurs zones de disponibilité (Availability Zones ou AZs).\nChaque zone de disponibilité est une partition entièrement isolée de l’infrastructure mondiale AWS.\nLes zones de disponibilité consistent en un ou plusieurs centres de données (typiquement 3). Elles sont isolées et conçues pour l’isolation des défaillances : Chaque zone de disponibilité a sa propre source d’alimentation et est physiquement séparée des autres zones de disponibilités. Elles sont interconnectées avec d’autres zones de disponibilité via des réseaux privés à haut débit : permet d’avoir une réplication syncrone et rapide des données (très peu de latence). Vous choisissez vos zones de disponibilité. AWS recommande de répliquer les données et les ressources dans minimum 2 zones de disponibilité pour garantir la résilience de vos services.\nExemple\nLa région Virginie du Nord possède 6 zones de disponibilité : us-east-1a, us-east-1b, us-east-1c, us-east-1d, us-east-1e, us-east-1f Une application s’exécute sur plusieurs zones : 1a, 1b et 1c, mais la zone 1a tombe en panne =\u003e votre application fonctionnera toujours dans les zones 1b et 1c. Centres de données Les centres de données AWS sont conçus pour la sécurité.\nLes centres de données sont l’emplacement où les données sont hébergées et où le traitement des données a lieu.\nChaque centre de données dispose d’une alimentation, d’un réseau et d’une connectivité redondants et est hébergé dans une installation distincte des autres centres de données.\nUn centre de données compte généralement entre 50 000 et 80 000 serveurs physiques !\nAvantages de l’infrastructure AWS Élasticité et mise à l’échelle (scalability) :\nInfrastructure élastique, adaptation dynamique de la capacité Infrastructure évolutive, adaptation à la croissance Tolérance aux pannes :\nFonctionnement continu en cas de panne Redondance intégrée des composants Haute disponibilité :\nHaut niveau de performances opérationnelles Temps d’arrêt réduit (down time) Aucune intervention humaine nécessaire Références Carte interactive de l’infrastructure mondiale de AWS Documentation Infrastructure mondiale AWS",
    "description": "AWS possède une infrastructure immense, avec des centres de données (data centers) déployés sur les quatre coins du globe. AWS a une façon spécifique de séparer logiquement et physiquement son infrastructure pour permettre une disponibilité et une redondance optimale.\nRégions Une région AWS est une zone géographique.\nVous contrôlez la réplication des données entre les régions. La communication entre les régions s’effecture par le biais de l’infrastructure réseau de AWS. Chaque région AWS assure une redondance et une connectivité complète au réseau AWS.",
    "tags": [],
    "title": "Infrastructure AWS",
    "uri": "/420-414/1-introduction/2-infrastructure-aws/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Réseau Infonuagique",
    "uri": "/420-414/2-reseau/index.html"
  },
  {
    "breadcrumb": "Réseau Infonuagique",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Notions",
    "uri": "/420-414/2-reseau/1-notions/index.html"
  },
  {
    "breadcrumb": "Réseau Infonuagique \u003e Notions",
    "content": "Définitions Définition simplifiée :\nUn VPC (Virtual Private Cloud) est une sous-section privée d’AWS que vous contrôlez et dans laquelle vous pouvez placer des ressources AWS (telles que des instances EC2 et des bases de données). Vous avez un contrôle total sur l’accès aux ressources AWS que vous déployez dans votre VPC. Définition d’AWS :\nAmazon Virtual Private Cloud (VPC) permet de mettre en service une section logiquement isolée du cloud AWS où vous pouvez lancer des ressources dans un réseau virtuel que vous définissez.\nAmazon VPC vous permet de contrôler vos ressources de réseau virtuel, notamment :\nla sélection d’une plage d’adresses IP la création de sous-réseaux la configuration de tables de routage et de passerelles réseau. Vous permet de personnaliser la configuration réseau de votre VPC.\nVous permet d’utiliser plusieurs couches de sécurité.\nNote Lorsque vous créez un compte AWS, un VPC “par défaut” est créé pour vous. Chaque VPC a une plage d’adresse définie. Chaque sous-réseau ou instance déployée dans le VPC aura une adresse IP incluse dans cette plage. VPC et sous-réseaux VPC :\nLogiquement isolées des autres VPC. Dédiés à votre compte AWS. Appartiennent à une seule région AWS et peuvent s’étendre sur plusieurs zones de disponibilité. Sous-réseaux :\nPlages d’adresses IP qui divient un VPC. Appartiennent à une seule zone de disponibilité. Classés comme publics ou privé Adressage IP Lorsque vous créez un VPC, vous l’affectez à un bloc d’adresse CIDR IPv4 (plage d’adresses IPv4 privées).\nUne fois que vous avez crée le VPC, nous ne pouvez plus modifier la plage d’adresses.\nLa plus grande taille de bloc d’adresse CIDR IPv4 est /16.\nLa plus petite taille de bloc d’adresse CIDR IPv4 est /28.\nIPv6 est également pris en charge.\nLes blocs d’adresses CIDR des sous-réseaux ne peuvent pas se chevaucher.\nAdresses réservées Exemple :\nUn VPC avec un bloc d’adresse CIDR IPv4 de 10.0.0.0/16 a 65 536 (2^16) adresses IP au total.\nLe VPC possède 4 sous-réseaux\n10.0.0.0/24 10.0.1.0/24 10.0.2.0/24 10.0.3.0/24 251 adresses IP sont disponibles par sous réseau (2^8 - 5 adresses réservées).\nTypes d’adresses IP publiques Adresse IP privées :\nOn attribue une adresse IP privée à ahaque machine déployée dans un sous-réseau. Cette adresse IP est inclue dans la plage d’adresses du sous-réseau. Adresse IPv4 publique :\nAttribuée manuellement via une adresse IP Elastic Attribuée automatiquement Adresse IP Elastic\nAssociée à un compte AWS Peut être allouée et remappée à tout moment Interface réseau Elastic Une interface réseau Elastic est une interface réseau virtuelle que vous pouvez :\nAttacher à une instance Détacher de l’instance et attacher à une autre instance pour rediriger le trafic réseau Ses attributs sont conservés lorsqu’elle est rattachée à une nouvelle instance.\nChaque instanbce de votre VPC possède une interface réseau par défaut à laquelle est attribuée une adresse IPv4 à partir de la plage d’adresses IPv4 de votre VPC.\nTables de routage et routes Une table de routage contient un ensemble de règles (ou routes) que vous pouvez configurer pour déterminer où le trafic réseau doit être dirigé depuis votre sous-réseau.\nChaque route spécifie une destination et une cible.\nPar défaut, chaque table de routage contient la route locale pour la communication au sein du VPC.\nChaque sous-réseau doit être associé à une table de routage (au plus une).\nNote Pensez une table de routage comme un GPS : Elle redirige les données vers la destination (adresse IP de destination). Votre VPC par défaut a déjà une table de routage principale.",
    "description": "Définitions Définition simplifiée :\nUn VPC (Virtual Private Cloud) est une sous-section privée d’AWS que vous contrôlez et dans laquelle vous pouvez placer des ressources AWS (telles que des instances EC2 et des bases de données). Vous avez un contrôle total sur l’accès aux ressources AWS que vous déployez dans votre VPC. Définition d’AWS :\nAmazon Virtual Private Cloud (VPC) permet de mettre en service une section logiquement isolée du cloud AWS où vous pouvez lancer des ressources dans un réseau virtuel que vous définissez.",
    "tags": [],
    "title": "Virtual Private Cloud",
    "uri": "/420-414/2-reseau/1-notions/1-vpc/index.html"
  },
  {
    "breadcrumb": "Réseau Infonuagique \u003e Notions",
    "content": "Passerelle Internet (Internet Gateway ou IGW) Définition simplifiée\nUne combinaison de matériel et de logiciel qui fournit à votre VPC une route vers le monde extérieur (c’est-à-dire l’Internet). Définition AWS\n“Une passerelle Internet est un composant VPC redondant et hautement disponible, mis à l’echelle horizontalement, qui permet la communication entre les instances de votre VPC et Internet. Elle n’impose donc aucun risque de disponibilité ni aucune contrainte de bande passante à votre trafic réseau. Note Votre VPC par défaut a déjà une passerelle internet attachée. On ne peut attacher qu’un IGW par VPC. Passerelle NAT Définition AWS\n“Une passerelle NAT est un service de traduction d’adresses réseau (NAT). Vous pouvez utiliser une passerelle NAT afin que les instances d’un sous-réseau privé puissent se connecter à des services en dehors de votre VPC (sur Internet), mais que les services externes ne puissent pas initier une connexion avec ces instances.”",
    "description": "Passerelle Internet (Internet Gateway ou IGW) Définition simplifiée\nUne combinaison de matériel et de logiciel qui fournit à votre VPC une route vers le monde extérieur (c’est-à-dire l’Internet). Définition AWS\n“Une passerelle Internet est un composant VPC redondant et hautement disponible, mis à l’echelle horizontalement, qui permet la communication entre les instances de votre VPC et Internet. Elle n’impose donc aucun risque de disponibilité ni aucune contrainte de bande passante à votre trafic réseau.",
    "tags": [],
    "title": "Passerelles",
    "uri": "/420-414/2-reseau/1-notions/2-passerelles/index.html"
  },
  {
    "breadcrumb": "Réseau Infonuagique \u003e Notions",
    "content": "Groupe de sécurité (Security Group ou SG) Pare-feu/couche de sécurité au niveau d’une instance (spécifiquement au niveau de l’interface/carte réseau).\nLes groupes de sécurité ont des règles qui contrôlent le trafic d’instance entrant (inbound) et sortant (outbound).\nLes groupes de sécurité par défaut refusent tout trafic entrant et autorisent tout le trafic sortant.\nLes groupes de sécurité sont avec état (stateful) : On ne définit que le trafic entrant, le trafic sortant est toujours autorisé.\nListe de contrôle d’accès réseau (Network ACLs ou NACL) Pare-feu/couche de sécurité qui contrôle le trafic entrant (inbound) et sortant (outbound) pour un ou plusieurs subnet(s) (au niveau du subnet).\nLes ACL réseau agissent au niveau du sous-réseau\nUne ACL réseau comporte des règles entrantes et sortantes distinctes.\nLes ACL réseau par défaut autorisent tout le trafic IPv4 entre et sortant.\nLes ACL réseau sont sans état (stateless) : Il n’enregistre pas les requêtes. Il faut donc définir des règles pour le trafic entrant ET sortant du sous-réseau.\nRègles de l’ACL réseau par défaut : Groupe de sécurité vs ACL réseau",
    "description": "Groupe de sécurité (Security Group ou SG) Pare-feu/couche de sécurité au niveau d’une instance (spécifiquement au niveau de l’interface/carte réseau).\nLes groupes de sécurité ont des règles qui contrôlent le trafic d’instance entrant (inbound) et sortant (outbound).\nLes groupes de sécurité par défaut refusent tout trafic entrant et autorisent tout le trafic sortant.\nLes groupes de sécurité sont avec état (stateful) : On ne définit que le trafic entrant, le trafic sortant est toujours autorisé.",
    "tags": [],
    "title": "Sécurité de VPC",
    "uri": "/420-414/2-reseau/1-notions/3-securite-vpc/index.html"
  },
  {
    "breadcrumb": "Réseau Infonuagique",
    "content": "Création d’un VPC et déploiement d’un serveur web Présentation et objectifs Dans cet atelier, vous allez utiliser Amazon Virtual Private Cloud (VPC) pour créer votre propre VPC et y ajouter des composants pour obtenir un réseau personnalisé. Ensuite, vous allez créer un groupe de sécurité, configurer et personnaliser une instance EC2 pour y exécuter un serveur web et lancer l’exécution de cette instance EC2 dans un sous-réseau du VPC.\nAmazon Virtual Private Cloud (Amazon VPC) vous permet de lancer des ressources Amazon Web Services (AWS) dans un réseau virtuel que vous définissez. Ce réseau virtuel ressemble beaucoup à un réseau classique que vous utiliseriez dans votre propre centre de données (data center), avec en plus l’avantage d’utiliser l’infrastructure d’AWS.\nÀ la fin de cet atelier, vous saurez :\nCréer un VPC ; Créer des sous-réseaux ; Configurer un groupe de sécurité ; Lancer une instance EC2 dans un VPC. Scénario Dans cet atelier, vous allez créer l’infrastructure suivante :\nÉtape 1 : Création d’un VPC Dans cette étape, vous allez utiliser la console VPC pour créer plusieurs ressources, notamment :\nUn VPC ; Une passerelle Internet (IGW) Un sous-réseau public et un sous-réseau privé dans une zone de disponibilité, Deux tables de routage Une passerelle NAT. Dans la zone de recherche à droite de Services, recherchez et choisissez VPC pour ouvrir la console VPC.\nCommencez la création d’un VPC :\nEn haut à droite de l’écran, vérifiez que la région est Virginie du Nord (us-east-1). Choisissez le lien Tableau de bord du VPC, qui se trouve également dans la partie supérieure gauche de la console. Choisissez ensuite Créer un VPC. Configurez les détails du VPC dans le volet Paramètres VPC à gauche :\nChoisissez VPC et plus encore. Sous Génération automatique d’identifications de noms, conservez l’option Génération automatique sélectionnée, mais remplacez la valeur project par lab. Conservez le bloc d’adresse CIDR IPv4 défini sur 10.0.0.0/16. Pour Nombre de zones de disponibilité, choisissez 1. Pour Nombre de sous-réseaux publics, conservez 1 Pour Nombre de sous-réseaux privés, conservez 1. Développez la section Personnaliser les blocs d’adresse CIDR des sous-réseaux. Remplacez Public subnet CIDR block in us-east-1a (Bloc d’adresse CIDR du sous-réseau public dans us-east-1a) par 10.0.0.0/24. Remplacez Private subnet CIDR block in us-east-1a (Bloc d’adresse CIDR du sous-réseau privé dans us-east-1a) par 10.0.1.0/24. Définissez Passerelles NAT sur In 1 AZ (Dans 1 AZ). Définissez Points de terminaison d’un VPC sur Aucun. Conservez les options Noms d’hôte DNS et Résolution DNS activées. Dans le volet Aperçu à droite, confirmez les paramètres que vous avez configurés :\nVPC : lab-vpc Sous-réseaux : us-east-1a Nom du sous-réseau public : lab-subnet-public1-us-east-1a Nom du sous-réseau privé : lab-subnet-private1-us-east-1a Tables de routage lab-rtb-public lab-rtb-private1-us-east-1a Connexions réseau lab-igw lab-nat-public1-us-east-1a En bas de l’écran, choisissez Créer un VPC.\nLes ressources VPC sont créées. L’activation de la passerelle NAT prend quelques minutes.\nAttendez que toutes les ressources soient créées avant de passer à l’étape suivante.\nUne fois l’opération terminée, choisissez Afficher le VPC.\nL’Assistant a mis en service un VPC avec un sous-réseau public et un sous-réseau privé dans une zone de disponibilité, avec des tables de routage pour chaque sous-réseau. Il a également créé une passerelle Internet et une passerelle NAT.\nPour afficher les paramètres de ces ressources, parcourez les liens de la console VPC qui affichent les détails des ressources. Par exemple, choisissez Sous-réseaux pour afficher les détails des sous-réseaux et choisissez Tables de routage pour afficher les détails des tables de routage. Le diagramme ci-dessous résume les ressources VPC que vous venez de créer et leur configuration :\nRappels et résumé Une passerelle Internet (IGW) est une ressource VPC qui autorise la communication entre les instances EC2 de votre VPC et Internet. Le sous-réseau public lab-subnet-public1-us-east-1a possède le bloc d’adresses CIDR 10.0.0.0/24, ce qui signifie qu’il contient toutes les adresses IP commençant par 10.0.0.x. Ce sous-réseau est public car la table de routage qui lui est associée achemine le trafic réseau 0.0.0.0/0 vers la passerelle Internet. Une passerelle NAT est une ressource VPC utilisée pour fournir une connectivité Internet aux instances EC2 exécutées dans les sous-réseaux privés du VPC sans que ces instances EC2 aient besoin d’une connexion directe à la passerelle Internet. Le sous-réseau privé lab-subnet-private1-us-east-1a possède le bloc d’adresses CIDR 10.0.1.0/24, ce qui signifie qu’il contient toutes les adresses IP commençant par 10.0.1.x. Étape 2 : Création de sous-réseaux supplémentaires Dans cette étape, vous allez créer deux sous-réseaux supplémentaires pour le VPC dans une deuxième zone de disponibilité. Il est utile d’avoir des sous-réseaux dans plusieurs zones de disponibilité d’un VPC pour déployer des solutions qui fournissent une haute disponibilité.\nDans l’étape précédente, vous avez crée un VPC, mais vous pouvez encore le configurer davantage, par exemple en ajoutant des sous-réseaux supplémentaires.\nDans le volet de navigation gauche, sélectionnez Sous-réseaux.\nTout d’abord, vous allez créer un deuxième sous-réseau public.\nChoisissez Créer un sous-réseau, puis configurez les éléments suivants :\nID de VPC : lab-vpc (sélectionnez-le dans le menu). Nom du sous-réseau : lab-subnet-public2 Zone de disponibilité : Sélectionnez une zone de disponibilité différente de la première (par exemple, us-east-1b). IPv4 CIDR block (Bloc d’adresse CIDR IPv4) : 10.0.2.0/24 Toutes les adresses IP du sous-réseau commenceront par 10.0.2.x.\nSélectionnez Créer un sous-réseau\nLe deuxième sous-réseau public a été créé. Vous allez maintenant créer un deuxième sous-réseau privé.\nChoisissez Créer un sous-réseau, puis configurez les éléments suivants :\nID de VPC : lab-vpc Nom du sous-réseau : lab-subnet-private2 Zone de disponibilité : sélectionnez la même zone de disponibilité que dans 2. (par exemple us-east-1b). IPv4 CIDR block (Bloc d’adresse CIDR IPv4) : 10.0.3.0/24 Toutes les adresses IP du sous-réseau commenceront par 10.0.3.x.\nSélectionnez Créer un sous-réseau\nLe deuxième sous-réseau privé a été créé.\nVous allez maintenant configurer ce nouveau sous-réseau privé pour router le trafic lié à Internet vers la passerelle NAT, afin que les ressources du deuxième sous-réseau privé puissent se connecter à Internet, tout en conservant les ressources privées. Pour ce faire, vous devez configurer une table de routage.\nUne table de routage contient un ensemble de règles, appelées acheminements, qui permettent de déterminer la direction du trafic réseau. Chaque sous-réseau d’un VPC doit être associé à une table de routage. Cette table de routage contrôle le routage pour le sous-réseau.\nDans le volet de navigation gauche, sélectionnez Tables de routage.\nSélectionnez la table de routage lab-rtb-private1-us-east-1a.\nDans le volet inférieur, choisissez l’onglet Routes.\nNotez que l’option Destination 0.0.0.0/0 est configurée sur Cible nat-xxxxxxxx. Cela signifie que le trafic destiné à Internet (0.0.0.0/0) sera envoyé à la passerelle NAT. La passerelle NAT transférera ensuite le trafic vers Internet.\nCette table de routage est donc utilisée pour acheminer le trafic à partir de sous-réseaux privés.\nSélectionnez l’onglet Associations de sous-réseau.\nVous avez créé cette table de routage dans l’étape 1 lorsque vous avez choisi de créer un VPC et plusieurs ressources dans ce VPC. Cette action a également créé lab-subnet-private-1 et associé ce sous-réseau à cette table de routage.\nMaintenant que vous avez créé un autre sous-réseau privé, lab-subnet-private-2, vous allez également lui associer cette table de routage.\nDans le volet Associations de sous-réseau explicites, choisissez Modifier les associations de sous-réseau.\nLaissez lab-subnet-private1-us-east-1a sélectionné, mais sélectionnez également lab-subnet-private2.\nSélectionnez Enregistrer les associations.\nVous allez maintenant configurer la table de routage utilisée par les sous-réseaux publics.\nSélectionnez la table de routage lab-rtb-public (et désélectionnez tout autre sous-réseau).\nDans le volet inférieur, choisissez l’onglet Routes.\nNotez que l’option Destination 0.0.0.0/0 est configurée sur Cible igw-xxxxxxxx, qui est une passerelle Internet. Cela signifie que le trafic lié à Internet sera envoyé à Internet via cette passerelle Internet.\nVous allez maintenant associer cette table de routage au deuxième sous-réseau public que vous avez créé.\nSélectionnez l’onglet Associations de sous-réseau.\nDans la zone Associations de sous-réseau explicites, choisissez Modifier les associations de sous-réseau.\nLaissez lab-subnet-public1-us-east-1a sélectionné, mais sélectionnez également lab-subnet-public2.\nSélectionnez Enregistrer les associations.\nVotre VPC dispose désormais de sous-réseaux publics et privés configurés dans deux zones de disponibilité. Les tables de routage que vous avez créées dans l’étape 1 ont également été mises à jour pour router le trafic réseau pour les deux nouveaux sous-réseaux.\nÉtape 3 : Création d’un groupe de sécurité Dans cette étape, vous allez créer un groupe de sécurité qui agit comme un pare-feu virtuel. Lorsque vous démarrez une instance (VM), vous lui associez un ou plusieurs groupes de sécurité. Vous pouvez ajouter des règles à chaque groupe de sécurité pour autoriser le trafic vers ou depuis ses instances associées.\nDans le volet de navigation gauche, sélectionnez Groupes de sécurité.\nChoisissez Créer un groupe de sécurité, puis configurez les paramètres suivants :\nNom du groupe de sécurité : Web Security Group Description : Enable HTTP access VPC : choisissez le symbole X pour supprimer le VPC actuellement sélectionné, puis choisissez lab-vpc dans la liste déroulante. Dans le volet Règles entrantes, choisissez Ajouter une règle.\nConfigurez les paramètres suivants :\nType : HTTP Source : Anywhere-Ipv4 Description : Permit Web Requests Faites défiler l’affichage jusqu’au bas de la page, puis choisissez Créer un groupe de sécurité.\nVous utiliserez ce groupe de sécurité dans le cadre de la prochaine étape, lors du lancement d’une instance Amazon EC2.\nÉtape 4 : Déploiement un serveur web Dans cette étape, vous allez lancer une instance Amazon EC2 dans le VPC configuré dans les étapes précédentes. Vous allez configurer l’instance pour qu’elle fonctionne en tant que serveur web.\nDans la zone de recherche, à droite de Services, recherchez et choisissez EC2 pour ouvrir la console EC2.\nDans le menu Lancer une instance, choisissez Lancer l’instance.\nNommez l’instance :\nAttribuez-lui le nom Web Server 1.\nLorsque vous nommez l’instance, AWS crée une identification et l’associe à l’instance. Une identification est une paire clé-valeur. La clé de cette paire est Nom et la valeur est le nom que vous saisissez pour votre instance EC2.\nChoisissez une AMI à partir de laquelle créer l’instance :\nDans la liste des AMI Quick Start disponibles, conservez la sélection par défaut d’Amazon Linux. Conservez également la sélection par défaut de l’AMI Amazon Linux 2023. Le type d’Amazon Machine Image (AMI) que vous choisissez détermine le système d’exploitation qui sera exécuté sur l’instance EC2 que vous lancez.\nChoisissez un type d’instance :\nDans le volet Type d’instance, conservez la valeur par défaut t2.micro sélectionnée.\nL’option Type d’instance définit les ressources matérielles affectées à l’instance.\nSélectionnez la paire de clés à associer à l’instance :\nDans le menu Nom de la paire de clés, sélectionnez vockey. La paire de clés vockey que vous avez sélectionnée vous permettra de vous connecter à cette instance via SSH après son lancement. Bien qu’il ne soit pas nécessaire d’exécuter cette opération dans ce laboratoire, vous devez toujours identifier une paire de clés existante, en créer une nouvelle ou choisir de continuer sans paire de clés lors du lancement d’une instance.\nConfigurez les paramètres réseau :\nDans la section Paramètres réseau, choisissez Modifier, puis configurez :\nRéseau : lab-vpc Sous-réseau : lab-subnet-public2 (non privé !) Attribuer automatiquement l’adresse IP publique : Activer Vous allez ensuite configurer l’instance de manière à utiliser le Groupe de sécurité web que vous avez créé précédemment.\nSous Pare-feu (groupes de sécurité), choisissez Sélectionner un groupe de sécurité existant.\nPour Groupes de sécurité courants, sélectionnez Groupe de sécurité web.\nCe groupe de sécurité permettra un accès HTTP à l’instance.\nDans la section Configurer le stockage, conservez les paramètres par défaut.\nRemarque : les paramètres par défaut indiquent que le volume racine de l’instance, qui hébergera le système d’exploitation invité Amazon Linux que vous avez spécifié précédemment, s’exécutera sur un disque dur SSD polyvalent (gp3) de 8 Gio. Vous pouvez également ajouter des volumes de stockage supplémentaires, mais cela n’est pas nécessaire dans cet atelier.\nConfigurez un script pour qu’il s’exécute sur l’instance à son lancement :\nDéveloppez le volet Détails avancés. Faites défiler la page jusqu’en bas, puis copiez et collez le code ci-dessous dans la zone Données utilisateur : #!/bin/bash # Install Apache Web Server and PHP dnf install -y httpd wget php mariadb105-server # Download Lab files wget https://aws-tc-largeobjects.s3.us-west-2.amazonaws.com/CUR-TF-100-ACCLFO-2/2-lab2-vpc/s3/lab-app.zip unzip lab-app.zip -d /var/www/html/ # Turn on web server chkconfig httpd on service httpd start Ce script s’exécutera avec des autorisations d’administrateur sur le système d’exploitation de l’instance. Il sera exécuté automatiquement lorsque l’instance sera lancée pour la première fois. Le script installe un serveur web, une base de données et des bibliothèques PHP, puis télécharge et installe une application web PHP sur le serveur web.\nEn bas du volet Résumé, à droite de l’écran, choisissez Lancer l’instance.\nUn message de réussite s’affiche.\nSélectionnez Afficher toutes les instances.\nAttendez que Web Server 1 affiche 2/2 checks passed (2/2 contrôles réussis) dans la colonne Contrôles des statuts.\nVous allez maintenant vous connecter au serveur web s’exécutant sur l’instance EC2.\nSélectionnez Web Server 1.\nCopiez la valeur de Public IPv4 DNS (DNS IPv4 public) indiquée dans l’onglet Détails au bas de la page.\nOuvrez un nouvel onglet de navigateur web, collez la valeur DNS public et appuyez sur Entrée.\nVous devriez alors voir une page web afficher le logo AWS et les valeurs de métadonnées d’instance.\nFélicitation, vous venez de déployer votre première infrastructure réseau et votre premier serveur Web sur le Cloud ! L’architecture complète que vous avez déployée est la suivante :\nConseils Pour vous assurer de maitriser les méthodes vues dans ce laboratoire, n’hésitez pas à le refaire sans l’aide de ce guide (la meilleure façon d’apprendre, c’est de répéter de façon autonome).",
    "description": "Création d’un VPC et déploiement d’un serveur web Présentation et objectifs Dans cet atelier, vous allez utiliser Amazon Virtual Private Cloud (VPC) pour créer votre propre VPC et y ajouter des composants pour obtenir un réseau personnalisé. Ensuite, vous allez créer un groupe de sécurité, configurer et personnaliser une instance EC2 pour y exécuter un serveur web et lancer l’exécution de cette instance EC2 dans un sous-réseau du VPC.\nAmazon Virtual Private Cloud (Amazon VPC) vous permet de lancer des ressources Amazon Web Services (AWS) dans un réseau virtuel que vous définissez.",
    "tags": [],
    "title": "Laboratoire",
    "uri": "/420-414/2-reseau/2-laboratoire/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Au cours des deux dernières décennies, la manière dont les entreprises gèrent leur infrastructure a beaucoup évolué. L’époque des serveurs physiques dédiés est révolue depuis longtemps, et il existe toute une série d’options pour tirer le meilleur parti de vos hôtes et services, que vous les déployez sur-site ou dans le nuage.\nLa virtualisation a ouvert la voie à la scalabilité, à la normalisation et à l’optimisation des coûts.\nLa conteneurisation a apporté de nouvelles efficacités.\nDans ce module, nous aborderons les spécificité de chacun des deux, plus spécifiquement :\nLa virtualisation : types, spécificités, avantages et inconvénients\nLa conteneurisation : types, spécificités, avantages et inconvénients\nDéploiement de machines virtuelles sur le Cloud AWS (EC2)\nIntroduction à Docker : +",
    "description": "Au cours des deux dernières décennies, la manière dont les entreprises gèrent leur infrastructure a beaucoup évolué. L’époque des serveurs physiques dédiés est révolue depuis longtemps, et il existe toute une série d’options pour tirer le meilleur parti de vos hôtes et services, que vous les déployez sur-site ou dans le nuage.\nLa virtualisation a ouvert la voie à la scalabilité, à la normalisation et à l’optimisation des coûts.\nLa conteneurisation a apporté de nouvelles efficacités.",
    "tags": [],
    "title": "VMs et Conteneurs",
    "uri": "/420-414/3-vm-conteneur/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs",
    "content": "Autrefois, les serveurs physiques fonctionnaient comme un ordinateur ordinaire. Vous disposiez d’un boîtier physique, vous installiez un système d’exploitation, puis vous installiez des applications par-dessus.\nCes types de serveurs sont appelés “serveurs bare-metal” (n’y a rien entre la machine physique et le système d’exploitation).\nCes serveurs étaient dédiés à un usage spécifique. La gestion était simple et les problèmes plus faciles à traiter mais les coûts étaient élevés : Il fallait de plus en plus de serveurs au fur et à mesure qu’une entreprise se développait.\nLa virtualisation existe depuis les années 1960, mais a commencé se développer au début des années 2000. Plutôt que de faire fonctionner le système d’exploitation directement sur le matériel physique, une couche de virtualisation supplémentaire est ajoutée entre les deux, ce qui permet de déployer plusieurs serveurs virtuels, chacun avec son propre système d’exploitation, le tout sur une seule machine physique.\nCela permet de réaliser des économies et une optimisation des ressources matérielles considérables, et a finalement conduit à l’existence du cloud computing.\nLe rôle d’un hyperviseur La virtualisation ne serait pas possible sans hyperviseurs - une couche logicielle qui permet à plusieurs machines virtuelles/systèmes d’exploitation de coexister tout en partageant les ressources d’un seul hôte matériel. L’hyperviseur sert d’intermédiaire entre les machines virtuelles et le matériel sous-jacent, en allouant les ressources de l’hôte telles que la mémoire, le processeur et le stockage à chacune des VMs.\nTypes d’hyperviseurs Il existe deux principaux types d’hyperviseurs :\nHyperviseurs de type 1 ou “bare-metal” : Ils s’exécutent directement sur le matériel de l’hôte et sont responsables de la gestion des ressources matérielles et de l’exécution des machines virtuelles. Comme ils s’exécutent directement sur le matériel, ils sont souvent plus efficaces que les hyperviseurs de type 2.\nExemples : VMware ESXi, Microsoft Hyper-V, Proxmox etc… Hyperviseurs de type 2 ou hébergés : Ils s’exécutent au-dessus d’un système d’exploitation hôte et s’appuient sur celui-ci pour fournir les ressources matérielles nécessaires. Comme ils s’exécutent au-dessus d’un système d’exploitation, ils sont souvent plus faciles à installer et à utiliser que les hyperviseurs de type 1, mais sont en général moins efficaces.\nExemples : VMware Workstation, Oracle VirtualBox etc… Avantages Réduction des coûts : En faisant tourner plusieurs machines virtuelles sur un seul serveur physique, on économise de l’argent et de l’espace. Meilleure utilisation des ressources et plus grande flexibilité : La possibilité d’exécuter plusieurs machines virtuelles sur un seul serveur permet d’éviter de gaspiller les capacités matérielles des serveurs. Facilite l’évolution, la gestion des ressources et les plans de reprise après une catastrophe : Il est possible de créer, détruire et migrer facilement des machines virtuelles entre hôtes. Inconvénients Peut créer une surcharge des performances : Introduit une couche supplémentaire entre l’hôte et le système d’exploitation. Ajoute un niveau de complexité : Il faut gérer et entretenir des instances physiques ET virtuelles.",
    "description": "Autrefois, les serveurs physiques fonctionnaient comme un ordinateur ordinaire. Vous disposiez d’un boîtier physique, vous installiez un système d’exploitation, puis vous installiez des applications par-dessus.\nCes types de serveurs sont appelés “serveurs bare-metal” (n’y a rien entre la machine physique et le système d’exploitation).\nCes serveurs étaient dédiés à un usage spécifique. La gestion était simple et les problèmes plus faciles à traiter mais les coûts étaient élevés : Il fallait de plus en plus de serveurs au fur et à mesure qu’une entreprise se développait.",
    "tags": [],
    "title": "Virtualisation",
    "uri": "/420-414/3-vm-conteneur/1-virtualisation/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs",
    "content": "Comme la virtualisation, la conteneurisation permet aussi d’exécuter de nombreuses instances sur un seul hôte physique, mais sans que l’hyperviseur ne joue le rôle d’intermédiaire.\nAu lieu de cela, la fonctionnalité du noyau du système hôte est utilisée pour isoler plusieurs instances indépendantes appelées conteneurs.\nEn partageant le noyau et le système d’exploitation de l’hôte, les conteneurs évitent le gaspillage de ressources matérielles de la virtualisation (il n’est pas nécessaire de fournir un noyau et un système d’exploitation virtuels différents pour chaque instance). C’est pourquoi les conteneurs sont considérés comme une solution plus légère - ils nécessitent moins de ressources sans compromettre les performances.\nTypes de conteneurs Il existe deux principaux types de conteneurs :\nConteneurs d’application (Docker) : Empaquettent et exécutent un “processus” (ou service) unique par conteneur. Ils sont emballés (ou conteneurisé) avec toutes les bibliothèques, dépendances et fichiers de configuration dont ils ont besoin pour fonctionner, ce qui facilite leur portabilité dans différents environnements. Conteneurs système (LXC) : Similaires à une machine virtuelle. Ils exécutent un système d’exploitation complet et ont le même comportement et la même facilité de gestion que les machines virtuelles tout en étant plus légers, avec en plus les avantages de densité et d’efficacité qu’offrent les conteneurs. Avantages Densité : Il est possible de faire fonctionner plusieurs conteneurs tout en bénéficiant des performances d’un système “bare-metal”. Efficacité : Permettent de déployer des applications beaucoup plus rapidement que des VMs. Portabilité : Toutes les dépendances sont déjà intégrées dans le conteneur. Inconvénients L’exécution de plusieurs conteneurs peut augmenter la complexité de l’environnement. Augmente la difficulté de surveillance et d’observabilité (dans un environnement avec des milliers de conteneurs). Toute vulnérabilité du noyau de l’hôte compromet tout ce qui s’exécute dans les conteneurs.",
    "description": "Comme la virtualisation, la conteneurisation permet aussi d’exécuter de nombreuses instances sur un seul hôte physique, mais sans que l’hyperviseur ne joue le rôle d’intermédiaire.\nAu lieu de cela, la fonctionnalité du noyau du système hôte est utilisée pour isoler plusieurs instances indépendantes appelées conteneurs.\nEn partageant le noyau et le système d’exploitation de l’hôte, les conteneurs évitent le gaspillage de ressources matérielles de la virtualisation (il n’est pas nécessaire de fournir un noyau et un système d’exploitation virtuels différents pour chaque instance).",
    "tags": [],
    "title": "Conteneurisation",
    "uri": "/420-414/3-vm-conteneur/2-conteneurisation/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs",
    "content": "Amazon Elastic Compute Cloud (Amazon EC2) est un service AWS qui permet de créer et gérer des machines virtuelles (appelées instances EC2) dans le cloud AWS.\nCréation d’une VM EC2 Les points les plus importants à configurer lorsque vous voulez lancer une instance EC2 :\nAMI Type d’instance Paramètres réseau Données utilisateur Options de stockage Groupe de sécurité Paire de clés AMI Amazon Machine Image (AMI) est un modèle (ou image) utilisé.e pour créer une instance EC2 Contient un système d’exploitation Windows ou Linux Certains logiciels sont souvent préinstallés Il est aussi possible de créer une AMI pour ensuite l’utiliser comme template. (comme un .ova ou un snapshot) Type d’instance Le type d’instance que vous choisissez dépend de vos besoins en :\nMémoire RAM Puissance de traitement (CPU) Espace disque et type de disque (stockage) Performances réseau Dénomination des types d’instances Exemple : t3.large\nt correspond au nom de la famille 3 correspond au numéro de génération large correspond à la taille Paramètres réseau Pour lancer une instance EC2, il faut spécifier le VPC et le sous-réseau où elle sera déployée. L’instance doit-elle avoir une adresse IP publique (pour la rendre accessible sur Internet) ? Si oui, il faut aussi le spécifier lors de la création. Données utilisateur (user-data) Vous pouvez (optionnellement) spécifier un script de données utilisateur (user data) au lancement de l’instance Le script s’exécute au démarrage initial de l’instance (avec les privilèges root) Options de stockage Il est nécessaire de configurer le volume racine (emplacement d’installation du système d’exploitation) Il est possible d’attacher des volumes de stockage supplémentaires (facultatif) Pour chaque volume, il est possible indiquez la taille du disque (en Go), le type de volume (SSD, HDD), si le volume doit être supprimé lorsque l’instance est détruite, si vous souhaitez chiffrer le volume Groupe de sécurité Rappel : Un groupe de sécurité est un ensemble de règles de pare-feu qui contrôlent le trafic entrant de l’instance. Paire de clés Au lancement de l’instance, vous devez indiquer une paire de clés existante ou en créer une Une paire de clés comprend : Une clé publique stockée par AWS Un fichier de clé privée que vous stockez Elle permet de sécuriser les connexions à une instance (via SSH pour les instances Linux, RDP pour les instances Windows) Interface de ligne de commande (CLI) Les instances EC2 (ainsi que d’autres services AWS) peuvent également être crées par programmation. Exemple de commande (suppose que la paire de clé et le groupe de sécurité existent déjà) :\naws ec2 run-instances / --image-id ami-1a2b3c4d / --count 1 / --instance-type c3.large / --key-name MyKeyPair / --security-groups MySecurityGroup --region us-east-1 Cycle de vie Le redémarrage d’une instance ne modifie aucune adresse IP ni aucun nom de domaine Lorsqu’une instance est arrêtée puis redémarrée : L’adresse IPv4 publique et le nom de domaine externe changent. L’adresse IPv4 privée et le nom de domaine interne ne change pas. Métadonnées Les métadonnées d’une instance sont des données relatives à une instance Vous pouvez les visualiser lorsque vous êtes connecté à l’instance : Dans un navigateur : http://169.254.169.254/latest/meta-data/ Dans un terminal : curl http://169.254.169.254/latest/meta-data/ Exemples de données : Adresse IP publique, privée, nom de domaine public, ID de l’instance, groupes de sécurité, région, zone de disponibilité. Les données utilisateur sont aussi accessibles : http://169.254.169.254/latest/user-data",
    "description": "Amazon Elastic Compute Cloud (Amazon EC2) est un service AWS qui permet de créer et gérer des machines virtuelles (appelées instances EC2) dans le cloud AWS.\nCréation d’une VM EC2 Les points les plus importants à configurer lorsque vous voulez lancer une instance EC2 :\nAMI Type d’instance Paramètres réseau Données utilisateur Options de stockage Groupe de sécurité Paire de clés AMI Amazon Machine Image (AMI) est un modèle (ou image) utilisé.",
    "tags": [],
    "title": "Amazon EC2",
    "uri": "/420-414/3-vm-conteneur/3-ec2/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Docker",
    "uri": "/420-414/3-vm-conteneur/4-docker/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs \u003e Docker",
    "content": "Vue d’ensemble et utilité Pour comprendre l’utilité de Docker, commençons par un exemple :\nImaginons que nous devions mettre en place un “stack” (plusieurs services pour faire rouler une application) incluant plusieurs technologies différentes comme un serveur Web utilisant Node.js (Express) une base de données (MongoDB) et un système de messagerie comme Redis.\nPlusieurs enjeux se posent :\nNous devons nous assurer que tous ces services soient compatibles avec la version du système d’exploitation qu’on prévoit utiliser. Dans certains cas, des versions de ces services peuvent ne pas être compatibles avec un OS mais sont compatibles avec un autre.\nIl faut aussi vérifier que les bibliothèques et les dépendances de ces services soient compatibles entre elles. En effet, un service pourrait avoir besoin d’une certaine version d’une bibliothèque pour fonctionner alors qu’un autre service ne peut fonctionner que sur une version différente de la même bibliothèque.\nImaginons maintenant que nous voulons changer l’infrastructure de l’application, comme par exemple mettre à jour un service ou changer la base de données vers MySQL. Chaque fois que quelque chose change dans notre stack, il faut passer par le même processus de vérification de la compatibilité entre les composants de notre application et l’infrastructure sous-jacente (OS).\nCe problème de matrice de compatibilité est appelé “la matrice de l’enfer” (Matrix From Hell).\nChaque fois qu’un nouveau développeur rentre dans une équipe, il est difficile de configurer un nouvel environnement pour lui qui satisfait toutes les dépendances nécessaires à faire rouler l’application : Il doit s’assurer d’avoir le bon OS, les bonnes versions de chacun des composants. Chaque développeur devait tout configurer lui-même à chaque fois. Il était donc impossible de garantir que l’application fonctionnerait de la même manière dans des environnements différents, ce qui rend notre vie dans le développement, le build et le shipping de l’application très difficile.\nIl faut donc trouver quelque chose qui pourrait aider avec ce problème de compatibilité, qui permettrait de modifier ou changer une composant sans affecter les autres et même modifier le système d’exploitation si nécessaire. C’est dans cette situation que Docker prend tout son sens.\nAvec Docker, il est possible d’exécuter chaque composant dans un conteneur séparé avec ses propres dépendances et ses propres bibliothèques, le tout sur la même VM et le même OS ! Une fois que Docker est installé, tous les développeurs peuvent lancer l’application avec une simple commande docker run, peu importe quel OS ils utilisent.\nFonctionnement Docker crée un environnement virtualisé pour une application appelée conteneur. Un conteneur est un package léger qui intègre tout ce dont vous avez besoin pour exécuter un service, y compris le code, les bibliothèques et les dépendances.\nLes conteneurs sont isolés du système hôte. Par conséquent, ils peuvent s’exécuter sur n’importe quelle machine prenant en charge Docker, quel que soit le système d’exploitation ou le matériel sous-jacent.\nLes conteneurs sont créés à partir d’images qui sont des modèles (ou templates) en lecture seule qui définissent l’application et ses dépendances. Ces images sont stockées dans un registre, tel que Docker Hub ou un registre privé. Les développeurs peuvent créer eux-mêmes des images personnalisées ou utiliser des images prédéfinies à partir du registre.\nLorsqu’un conteneur est lancé, il est construit à partir d’une image et doté de son propre système de fichiers, réseau et espace de processus isolés. Le conteneur peut alors exécuter l’application comme si elle s’exécutait sur un serveur dédié.\nWorkflow Docker Build : Créer un Dockerfile qui définit votre application et ses dépendances. Ship : Push votre image Docker vers un registre comme Docker Hub. Run : Télécharger l’image et l’exécuter sur n’importe quel hôte compatible avec Docker. Exemple :\n# Build une image docker build -t myapp:v1 . # Pull l'image vers Docker Hub username/myapp:v1 # Exécuter le conteneur docker run -d -p 8080:80 username/myapp:v1 Images Une image Docker est un package léger, autonome et exécutable qui conserve tout ce qui est nécessaire pour exécuter une application, y compris le code, les bibliothèques et les dépendances. Les images Docker sont utilisées pour créer et exécuter des conteneurs, qui sont des environnements isolés pouvant être utilisés pour exécuter des applications.\nLes images Docker sont créées à partir d’un Dockerfile, qui est un fichier texte contenant un ensemble d’instructions pour créer l’image. Le Dockerfile spécifie l’image de base, le code de l’application et ses dépendances, les variables d’environnement et d’autres options de configuration nécessaires à la création de l’image.\nLes images Docker sont stockées dans un registre public, tel que Docker Hub, ou privé. Chaque fois qu’un conteneur est créé à partir d’une image, il s’exécute en tant que processus distinct sur la machine hôte, isolé des autres processus et conteneurs.\nLes images Docker peuvent être utilisées pour déployer des applications de manière cohérente sur différentes plates-formes. Ils facilitent l’empaquetage (build), la distribution (shipping) et le déploiement (deploiment) d’applications et garantissent qu’elles fonctionnent de la même manière partout.\nConteneurs Un conteneur est une instance en cours d’exécution d’une image.\nChaque conteneur s’exécute comme un processus distinct sur la machine hôte et possède son propre système de fichiers, son réseau et d’autres ressources.\nLes conteneurs Docker sont conçus pour être portables et faciles à déployer. Ils peuvent être exécutés sur n’importe quelle machine sur laquelle Docker est installé, quel que soit le système d’exploitation ou le matériel sous-jacent. Les conteneurs fournissent un environnement cohérent pour l’exécution des applications, ce qui facilite le déplacement d’applications entre différents environnements, tels que le développement, les tests et la production.\nLes conteneurs Docker peuvent être gérés à l’aide de Docker CLI ou d’outils Docker tels que Docker Compose ou Kubernetes. Ils peuvent être démarrés, arrêtés, mis en pause et redémarrés selon les besoins. Ils peuvent également être surveillés et gérés à l’aide d’une gamme d’outils et de plateformes.\nDans l’ensemble, les conteneurs Docker offrent un moyen flexible et évolutif de regrouper et de déployer des applications, ce qui simplifie la gestion et la mise à l’échelle d’applications complexes sur différents environnements et plates-formes.\nRegistres d’images Docker Docker héberge l’un des plus grands registre d’images Docker, appelé Docker Hub.\nIl s’agit d’un système de stockage et de distribution d’images Docker. Il fournit un référentiel central permettant aux développeurs et aux organisations de partager et de distribuer leurs images Docker, ce qui rend plus agréable la création, le partage et le déploiement d’applications avec Docker.\nDocker Hub permet aux utilisateurs et aux organisations de stocker et de gérer leurs images Docker et fournit également des fonctionnalités telles que la gestion des versions, le balisage et la collaboration. Les utilisateurs peuvent rechercher et télécharger des images depuis Docker Hub, ainsi que publier leurs propres images dans le registre.\nEn plus du registre public, Docker Hub fournit un registre privé pour les organisations qui souhaitent gérer leurs propres images Docker et garantir qu’elles ne sont accessibles qu’aux utilisateurs autorisés.\nAvantages Portabilité : Les conteneurs Docker peuvent être exécutés sur n’importe quelle machine prenant en charge Docker, ce qui facilite le déploiement d’applications dans différents environnements.\nCohérence : En regroupant une application et ses dépendances dans un conteneur, Docker garantit que l’application s’exécutera de manière cohérente, quelle que soit l’infrastructure sous-jacente.\nÉvolutivité : Docker facilite la mise à l’échelle des applications horizontalement en exécutant plusieurs instances du même conteneur.\nEfficacité des ressources : Les conteneurs Docker sont légers et nécessitent un minimum de ressources, ce qui les rend idéaux pour une exécution sur une infrastructure cloud.\nSecurité : Docker fournit un environnement sécurisé et isolé pour l’exécution des applications, réduisant ainsi le risque de conflits avec d’autres applications ou avec le système hôte.",
    "description": "Vue d’ensemble et utilité Pour comprendre l’utilité de Docker, commençons par un exemple :\nImaginons que nous devions mettre en place un “stack” (plusieurs services pour faire rouler une application) incluant plusieurs technologies différentes comme un serveur Web utilisant Node.js (Express) une base de données (MongoDB) et un système de messagerie comme Redis.\nPlusieurs enjeux se posent :\nNous devons nous assurer que tous ces services soient compatibles avec la version du système d’exploitation qu’on prévoit utiliser.",
    "tags": [],
    "title": "Introduction",
    "uri": "/420-414/3-vm-conteneur/4-docker/1-introduction/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs \u003e Docker",
    "content": "Lancer votre premier conteneur docker run hello-world Cette commande effectue les opérations suivantes :\nRecherche de l’image hello-world localement Si l’image n’est pas trouvée, elle est téléchargée de Docker Hub Crée un conteneur à partir de l’image Exécute le conteneur, qui affiche un message d’accueil. Quitte le conteneur Commandes de base Lister les conteneurs Pour voir tous les conteneurs en cours d’exécution : docker ps Pour lister tous les conteneurs (même ceux arrêtés): docker ps -a Lancer et arrêter des conteneurs Arrêter un conteneur docker stop \u003cid_ou_nom_du_conteneur\u003e Démarrer un conteneur arrêté docker start \u003cid_ou_nom_du_conteneur\u003e Redémarrer un conteneur docker restart \u003cid_ou_nom_du_conteneur\u003e Supprimer des conteneurs Supprimer un conteneur arrêté docker rm \u003cid_ou_nom_du_conteneur\u003e Forcer la suppression d’un conteneur (en cours d’exécution) docker rm -f \u003cid_ou_nom_du_conteneur\u003e Modes de démarrage d’un conteneur Mode détaché (Detached Mode) Pour rouler un conteneur en arrière-plan : docker run -d \u003cimage\u003e Mode interactif (Interactive Mode) Pour rouler un conteneur et intéragir avec (entrer dans le conteneur) docker run -it \u003cnom_image\u003e /bin/bash Mappage de port Pour mapper le port d’un conteneur à celui de l’hôte docker run -p \u003cport_hote\u003e:\u003cport_conteneur\u003e \u003cimage\u003e Exemple : docker run -d -p 80:80 nginx Journaux des conteneurs (logs) Accéder aux journal d’un conteneur docker logs \u003cid_ou_nom_conteneur\u003e Suivre les logs d’un conteneur en temps-réel docker logs -f \u003cid_ou_nom_conteneur\u003e Exécuter des commandes dans un conteneur en cours d’exécution Exécuter une commande dans un conteneur en cours d’exécution : docker exec -it \u003cid_ou_nom_conteneur\u003e \u003ccommande\u003e Exemple : docker exec -it my_container /bin/bash Laboratoire : exécuter un conteneur Apache Exécutons un serveur web Apache dans un conteneur :\nTélécharger (pull) l’image : docker pull httpd Exécutez le conteneur : docker run -d --name my-apache -p 8080:80 httpd Vérifiez qu’il fonctionne : docker ps Accédez à la page par défaut en ouvrant un navigateur web et en naviguant vers http://localhost:8080 Modifiez la page par défaut : docker exec -it my-apache /bin/bash echo \"\u003ch1\u003eHello from my Apache container!\u003c/h1\u003e\" \u003e /usr/local/apache2/htdocs/index.html exit Rafraîchissez votre navigateur pour voir les changements. Mise en réseau des conteneurs Lister les réseaux docker network ls Créer un réseau docker create network my_network Attacher un conteneur à un réseau docker run -d --network my_network --name my_container \u003cimage\u003e Persistence des données avec les volumes Créer un volume docker volume create my_volume Exécuter un conteneur avec un volume docker run -d -v my_volume:/path/in/container \u003cimage_name\u003e Nettoyage Supprimer tous les conteneurs arrêtés docker container prune Supprimer toutes les ressources inutilisées (conteneurs, réseaux, images): docker system prune",
    "description": "Lancer votre premier conteneur docker run hello-world Cette commande effectue les opérations suivantes :\nRecherche de l’image hello-world localement Si l’image n’est pas trouvée, elle est téléchargée de Docker Hub Crée un conteneur à partir de l’image Exécute le conteneur, qui affiche un message d’accueil. Quitte le conteneur Commandes de base Lister les conteneurs Pour voir tous les conteneurs en cours d’exécution : docker ps Pour lister tous les conteneurs (même ceux arrêtés): docker ps -a Lancer et arrêter des conteneurs Arrêter un conteneur docker stop \u003cid_ou_nom_du_conteneur\u003e Démarrer un conteneur arrêté docker start \u003cid_ou_nom_du_conteneur\u003e Redémarrer un conteneur docker restart \u003cid_ou_nom_du_conteneur\u003e Supprimer des conteneurs Supprimer un conteneur arrêté docker rm \u003cid_ou_nom_du_conteneur\u003e Forcer la suppression d’un conteneur (en cours d’exécution) docker rm -f \u003cid_ou_nom_du_conteneur\u003e Modes de démarrage d’un conteneur Mode détaché (Detached Mode) Pour rouler un conteneur en arrière-plan : docker run -d \u003cimage\u003e Mode interactif (Interactive Mode) Pour rouler un conteneur et intéragir avec (entrer dans le conteneur) docker run -it \u003cnom_image\u003e /bin/bash Mappage de port Pour mapper le port d’un conteneur à celui de l’hôte docker run -p \u003cport_hote\u003e:\u003cport_conteneur\u003e \u003cimage\u003e Exemple : docker run -d -p 80:80 nginx Journaux des conteneurs (logs) Accéder aux journal d’un conteneur docker logs \u003cid_ou_nom_conteneur\u003e Suivre les logs d’un conteneur en temps-réel docker logs -f \u003cid_ou_nom_conteneur\u003e Exécuter des commandes dans un conteneur en cours d’exécution Exécuter une commande dans un conteneur en cours d’exécution : docker exec -it \u003cid_ou_nom_conteneur\u003e \u003ccommande\u003e Exemple : docker exec -it my_container /bin/bash Laboratoire : exécuter un conteneur Apache Exécutons un serveur web Apache dans un conteneur :",
    "tags": [],
    "title": "Commandes",
    "uri": "/420-414/3-vm-conteneur/4-docker/2-commandes/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs \u003e Docker",
    "content": "Concepts clés Couches (layers) : Les images sont composées de plusieurs couches, chacune représentant un ensemble de modifications du système de fichiers. modifications du système de fichiers. Image de base (Base Image) : La base d’une image, généralement un système d’exploitation minimal. Image parent (Parent Image) : Une image sur laquelle votre image est construite. Tags d’images : Étiquettes utilisées pour la version et l’identification des images. ID de l’image : Identifiant unique pour chaque image. Travailler avec des images Docker Pour voir toutes les images disponibles sur votre système local : docker images Ou utilisez la commande la plus verbeuse : docker image ls Pour télécharger (pull) des images de Docker Hub docker pull \u003cnom_image\u003e:\u003ctag\u003e Exemple : docker pull ubuntu:20.04 Si aucun tag n’est spécifié, Docker téléchargera la dernière version (latest) par défaut.\nExécution de conteneurs à partir d’images docker run \u003cnom_image\u003e:\u003ctag\u003e Exemple : docker run -it ubuntu:20.04 /bin/bash Informations sur l’image Pour obtenir des informations détaillées sur une image : docker inspect \u003cnom_image\u003e:\u003ctag\u003e Pour supprimer une image : docker rmi \u003cnom_image\u003e:\u003ctag\u003e ou\ndocker image rm \u003cnom_image\u003e:\u003ctag\u003e Pour supprimer toutes les images inutilisées : docker image prune Construire des images personnalisées En utilisant un Dockerfile Créer un fichier nommé Dockerfile (sans extension). Définir les instructions pour construire votre image. Exemple :\nFROM ubuntu:20.04 RUN apt-get update \u0026\u0026 apt-get install -y nginx COPY ./my-nginx.conf /etc/nginx/nginx.conf EXPOSE 80 CMD [\"nginx\", \"-g\", \"daemon off ;\"] Build l’image : docker build -t my-nginx:v1 . À partir d’un conteneur en cours d’exécution Apportez des modifications à un conteneur en cours d’exécution. Créer une nouvelle image à partir du conteneur : docker commit \u003ccontainer_id\u003e my-new-image:tag Tags d’images Pour tagger une image existante docker tag \u003cimage_source\u003e:\u003ctag\u003e \u003cimage_cible\u003e:\u003ctag\u003e Exemple : docker tag my-nginx:v1 my-dockerhub-username/my-nginx:v1 Téléverser des images sur Docker Hub Se connecter à Docker Hub docker login Téléverser (push) l’image docker push my-dockerhub-username/my-nginx:v1 Couches et mise en cache Il est essentiel de comprendre les couches pour optimiser la création d’images :\nChaque instruction d’un Dockerfile crée une nouvelle couche. Les couches sont mises en cache et réutilisées dans les prochains build. Le fait d’ordonner les instructions du moins au plus souvent modifiées peut accélérer les builds. Exemple de mise en cache : FROM ubuntu:20.04 RUN apt-get update \u0026\u0026 apt-get install -y nginx COPY ./static-files /var/www/html COPY ./config-files /etc/nginx Analyse d’images et sécurité Docker offre des fonctionnalités intégrées pour analyser vos images et identifier les vulnérabilités :\ndocker scan \u003cnom_image\u003e:\u003ctag\u003e",
    "description": "Concepts clés Couches (layers) : Les images sont composées de plusieurs couches, chacune représentant un ensemble de modifications du système de fichiers. modifications du système de fichiers. Image de base (Base Image) : La base d’une image, généralement un système d’exploitation minimal. Image parent (Parent Image) : Une image sur laquelle votre image est construite. Tags d’images : Étiquettes utilisées pour la version et l’identification des images. ID de l’image : Identifiant unique pour chaque image.",
    "tags": [],
    "title": "Images",
    "uri": "/420-414/3-vm-conteneur/4-docker/3-images/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs \u003e Docker",
    "content": "Un Dockerfile est un fichier qui contient une série d’instructions et d’arguments. Ces instructions sont utilisées pour créer automatiquement une image Docker. Il s’agit essentiellement d’un script de commandes successives que Docker exécutera pour assembler une image, automatisant ainsi le processus de création d’image.\nInstructions d’un Dockerfile FROM L’instruction FROM définit l’image de base pour les instructions suivantes.\nFROM ubuntu:24.04 Cette instruction est généralement la première d’un fichier Docker.\nLABEL LABEL ajoute des métadonnées à une image sous forme de paires clé-valeur.\nLABEL version=\"1.0\" maintainer=\"john@example.com\" description=\"Ceci est un exemple d'image Docker » Les étiquettes sont utiles pour l’organisation des images, les annotations ou autres métadonnées.\nENV ENV définit les variables d’environnement dans l’image.\nENV APP_HOME=/app NODE_ENV=production WORKDIR WORKDIR définit le répertoire de travail pour les instructions RUN, CMD, ENTRYPOINT, COPY et ADD subséquentes.\nWORKDIR /app Si le répertoire n’existe pas, il sera crée.\nCOPY et ADD Les instructions COPY et ADD copient les fichiers de l’hôte dans l’image.\nCOPY package.json . ADD https://example.com/big.tar.xz /usr/src/things/ COPY est généralement préféré pour sa simplicité. ADD possède quelques fonctionnalités supplémentaires comme l’extraction de fichiers compressés tar la prise en charge des URL distantes, mais celles-ci peuvent rendre la compilation moins prévisible.\nRUN RUN exécute des commandes dans une nouvelle couche au dessus de l’image courante et commet les résultats.\nRUN apt-get update \u0026\u0026 apt-get install -y nodejs Il est conseillé d’enchaîner les commandes avec \u0026\u0026 afin de réduire le nombre de couches.\nCMD CMD fournit la commande par défaut pour un conteneur en cours d’exécution. Il ne peut y avoir qu’une seule instruction CMD dans un Dockerfile.\nCMD [\"node\", \"app.js\"] ENTRYPOINT ENTRYPOINT configure un conteneur qui s’exécutera en tant qu’exécutable.\nENTRYPOINT [\"nginx\", \"-g\", \"daemon off ;\"] ENTRYPOINT est souvent utilisé en combinaison avec CMD : ENTRYPOINT définit l’exécutable et CMD fournit les arguments par défaut.\nEXPOSE EXPOSE informe Docker que le conteneur écoute sur les ports réseau spécifiés au moment de l’exécution.\nEXPOSE 80 443 Cela n’expose pas réellement le port. Cette commande fonctionne comme une documentation entre la personne qui construit l’image et la personne qui exécute le conteneur.\nVOLUME VOLUME crée un point de montage et le marque comme contenant des volumes montés en externe de l’hôte natif ou d’autres conteneurs.\nVOLUME /data Ceci est utile pour toutes les parties mutables et/ou utilisables par l’utilisateur de votre image.\nARG ARG définit une variable que les utilisateurs peuvent passer au moment de la construction au constructeur avec la commande docker build.\nARG VERSION=latest Cela permet de faire des builds plus flexibles.\nDockerfile : bonnes pratiques Réduire au minimum le nombre de couches : Combinez les commandes lorsque cela est possible afin de réduire le nombre de couches et la taille de l’image. Exploiter le cache de construction : Classer les instructions par ordre décroissant de fréquence afin de maximiser l’utilisation de la mémoire cache. Utiliser .dockerignore : Exclure les fichiers non pertinents pour la construction, similaire à .gitignore. N’installez pas de paquets inutiles : Gardez l’image légère et sécurisée en n’installant que ce qui est nécessaire. Utilisez des balises spécifiques : Évitez la balise latest pour les images de base afin d’assurer des constructions reproductibles. Définissez le répertoire WORKDIR : utilisez toujours WORKDIR au lieu de proliférer des instructions telles que RUN cd … \u0026\u0026 do-something. Utilisez COPY au lieu de ADD : À moins que vous n’ayez explicitement besoin de la fonctionnalité supplémentaire de ADD, utilisez COPY pour la transparence. Utiliser les variables d’environnement : En particulier pour les numéros de version et les chemins d’accès, ce qui rend le fichier Docker plus flexible.",
    "description": "Un Dockerfile est un fichier qui contient une série d’instructions et d’arguments. Ces instructions sont utilisées pour créer automatiquement une image Docker. Il s’agit essentiellement d’un script de commandes successives que Docker exécutera pour assembler une image, automatisant ainsi le processus de création d’image.\nInstructions d’un Dockerfile FROM L’instruction FROM définit l’image de base pour les instructions suivantes.\nFROM ubuntu:24.04 Cette instruction est généralement la première d’un fichier Docker.\nLABEL LABEL ajoute des métadonnées à une image sous forme de paires clé-valeur.",
    "tags": [],
    "title": "Dockerfile",
    "uri": "/420-414/3-vm-conteneur/4-docker/4-dockerfile/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs \u003e Docker",
    "content": "La mise en réseau Docker permet aux conteneurs de communiquer entre eux et avec le monde extérieur.\nPilotes réseau Docker Docker propose plusieurs pilotes de réseau :\nBridge : Le pilote réseau par défaut. Il convient aux conteneurs autonomes qui ont besoin de communiquer. Host (hôte) : Supprime l’isolation réseau entre le conteneur et l’hôte Docker. Overlay (superposition) : Permet la communication entre les conteneurs sur plusieurs hôtes de démon Docker. MacVLAN : Attribue une adresse MAC à un conteneur, le faisant apparaître comme un périphérique physique sur le réseau. None (Aucun) : Désactive toute mise en réseau pour un conteneur. Network plugins (plugins réseau) : Permet d’utiliser des pilotes réseau tiers. Travailler avec les réseaux Docker Lister les réseaux docker network ls Cette commande affiche l’ID, le nom, le pilote et la portée de chaque réseau.\nInspecter les réseaux Pour obtenir des informations détaillées sur un réseau :\ndocker network inspect \u003cnom_du_réseau\u003e Cette commande fournit des informations telles que le sous-réseau du réseau, la passerelle, les conteneurs connectés et les options de configuration.\nCréation d’un réseau Pour créer un nouveau réseau :\ndocker network create --driver \u003cdriver\u003e \u003cnom_du_réseau\u003e Exemple :\ndocker network create --driver bridge my_custom_network Vous pouvez spécifier des options supplémentaires comme le sous-réseau, la passerelle, la plage IP, etc :\ndocker network create --driver bridge --subnet 172.18.0.0/16 --gateway 172.18.0.1 my_custom_network Connecter un conteneur à un réseaux Lors de l’exécution d’un conteneur, vous pouvez spécifier le réseau auquel il doit se connecter :\ndocker run --network \u003cnom_du_réseau\u003e \u003cimage\u003e Exemple :\ndocker run --network my_custom_network --name container1 -d nginx Vous pouvez également connecter un conteneur en cours d’exécution à un réseau :\ndocker network connect \u003cnom_du_réseau\u003e \u003cnom_du_conteneur\u003e Déconnecter un conteneur d’un réseau Pour déconnecter un conteneur d’un réseau :\ndocker network disconnect \u003cnom_du_réseau\u003e \u003cnom_du_conteneur\u003e. Supprimer un réseau Pour supprimer un réseau :\ndocker network rm \u003cnom_du_réseau\u003e",
    "description": "La mise en réseau Docker permet aux conteneurs de communiquer entre eux et avec le monde extérieur.\nPilotes réseau Docker Docker propose plusieurs pilotes de réseau :\nBridge : Le pilote réseau par défaut. Il convient aux conteneurs autonomes qui ont besoin de communiquer. Host (hôte) : Supprime l’isolation réseau entre le conteneur et l’hôte Docker. Overlay (superposition) : Permet la communication entre les conteneurs sur plusieurs hôtes de démon Docker.",
    "tags": [],
    "title": "Réseau",
    "uri": "/420-414/3-vm-conteneur/4-docker/5-reseau/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs \u003e Docker",
    "content": "Les volumes Docker sont le mécanisme privilégié pour conserver les données générées et utilisées par les conteneurs Docker. Bien que les conteneurs puissent créer, mettre à jour et supprimer des fichiers, ces modifications sont perdues lorsque le conteneur est supprimé et toutes les modifications sont isolées de ce conteneur. Les volumes permettent de connecter des chemins spécifiques du système de fichiers du conteneur à la machine hôte. Si un répertoire du conteneur est monté, les modifications apportées à ce répertoire sont également visibles sur la machine hôte. Si nous montons ce même répertoire à chaque redémarrage du conteneur, nous verrons les mêmes fichiers.\nUtilité des volumes Docker Persistance des données : Les volumes vous permettent de conserver les données même lorsque les conteneurs sont arrêtés ou supprimés. Partage des données : Les volumes peuvent être partagés et réutilisés entre plusieurs conteneurs. Performance : Les volumes sont stockés sur le système de fichiers de l’hôte, ce qui offre généralement de meilleures performances d’E/S, en particulier pour les bases de données. Gestion des données : Les volumes facilitent la sauvegarde, la restauration et la migration des données. Découplage : Les volumes découplent la configuration de l’hôte Docker de l’exécution du conteneur. Types de volumes Docker Volumes nommés Les volumes nommés (named volumes) sont le moyen recommandé pour conserver les données dans Docker. Ils sont créés explicitement et reçoivent un nom.\nCréation d’un volume nommé : docker volume create my_volume Utiliser un volume nommé : docker run -d --name devtest -v mon_volume:/app nginx:latest Volumes anonymes Les volumes anonymes sont automatiquement créés par Docker et reçoivent un nom aléatoire. Ils sont utiles pour les données temporaires que vous n’avez pas besoin de conserver au-delà de la durée de vie du conteneur.\nUtilisation d’un volume anonyme : docker run -d --name devtest -v /app nginx:latest Bind mounts Les Bind mounts font correspondre un chemin spécifique de la machine hôte à un chemin dans le conteneur. Ils sont utiles pour les environnements de développement.\nUtilisation d’un bind mount : docker run -d --name devtest -v /path/on/host:/app nginx:latest Travailler avec les volumes Docker Lister les volumes Pour lister tous les volumes :\ndocker volume ls Inspecter les volumes Pour obtenir des informations détaillées sur un volume :\ndocker volume inspect my_volume Supprimer des volumes Pour supprimer un volume spécifique :\ndocker volume rm mon_volume Pour supprimer tous les volumes inutilisés :\ndocker volume prune",
    "description": "Les volumes Docker sont le mécanisme privilégié pour conserver les données générées et utilisées par les conteneurs Docker. Bien que les conteneurs puissent créer, mettre à jour et supprimer des fichiers, ces modifications sont perdues lorsque le conteneur est supprimé et toutes les modifications sont isolées de ce conteneur. Les volumes permettent de connecter des chemins spécifiques du système de fichiers du conteneur à la machine hôte. Si un répertoire du conteneur est monté, les modifications apportées à ce répertoire sont également visibles sur la machine hôte.",
    "tags": [],
    "title": "Volumes",
    "uri": "/420-414/3-vm-conteneur/4-docker/6-volumes/index.html"
  },
  {
    "breadcrumb": "VMs et Conteneurs \u003e Docker",
    "content": "Docker Compose est un outil puissant pour définir et exécuter des applications Docker multi-conteneurs. Avec Docker Compose, vous utilisez un fichier YAML pour configurer les services, les réseaux et les volumes de votre application. Ensuite, vous pouvez créer et démarrer tous les services avec une seule commande.\nAvantages Simplicité : Définissez l’ensemble de votre stack d’applications dans un seul fichier. Reproductibilité : Partagez et contrôlez facilement la version de votre configuration d’application.",
    "description": "Docker Compose est un outil puissant pour définir et exécuter des applications Docker multi-conteneurs. Avec Docker Compose, vous utilisez un fichier YAML pour configurer les services, les réseaux et les volumes de votre application. Ensuite, vous pouvez créer et démarrer tous les services avec une seule commande.\nAvantages Simplicité : Définissez l’ensemble de votre stack d’applications dans un seul fichier. Reproductibilité : Partagez et contrôlez facilement la version de votre configuration d’application.",
    "tags": [],
    "title": "Docker Compose",
    "uri": "/420-414/3-vm-conteneur/4-docker/7-docker-compose/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/420-414/categories/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/420-414/tags/index.html"
  }
]
